__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
import streamlit as st
import google.generativeai as genai
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage
import os
from dotenv import load_dotenv
import io
import base64

load_dotenv()

# ------------------- ENV CONFIG -------------------
os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')
os.environ['LANGCHAIN_API_KEY'] = os.getenv("LANGCHAIN_API_KEY")
os.environ['LANGCHAIN_TRACING_V2'] = "true"
os.environ['LANGCHAIN_PROJECT'] = "ALL CHATBOT"

genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
groq_api_key = os.getenv('GROQ_API_KEY')


embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2",model_kwargs={'device': 'cpu'})

# ------------------- LLM INSTANCE GENERATOR -------------------
def get_llm_instance(llm_name, temp):
Â  Â  """Returns an instance of the selected LLM."""
Â  Â  if "gemini" in llm_name:
Â  Â  Â  Â  return ChatGoogleGenerativeAI(model=llm_name, temperature=temp)
Â  Â  elif "openai/gpt-oss" in llm_name:
Â  Â  Â  Â  return ChatGroq(model_name=llm_name, groq_api_key=groq_api_key, temperature=temp)
Â  Â  elif "llama" in llm_name or "gemma" in llm_name or "deepseek" in llm_name:
Â  Â  Â  Â  return ChatGroq(model_name=llm_name, groq_api_key=groq_api_key, temperature=temp)
Â  Â  else:
Â  Â  Â  Â  st.error("Selected model is not supported yet.")
Â  Â  Â  Â  return None

# ------------------- STREAMLIT UI -------------------
st.set_page_config(page_title="ðŸ§  Enchance Chatbot")
st.title("ðŸ§  Enchance Chatbot")
st.sidebar.title("Settings")

mode = st.sidebar.radio("Choose Mode", ("Normal", "PDF", "Image"))

if mode == "Image":
Â  Â  st.sidebar.info("Image analysis requires a Gemini (multimodal) model.")
Â  Â  model_choices = ["gemini-2.5-flash", "gemini-2.5-pro"]
else:
Â  Â  model_choices = ["gemini-flash-latest","gemini-2.5-pro","deepseek-r1-distill-llama-70b","llama-3.1-8b-instant","llama-3.1-70b-versatile","gemma2-9b-it","openai/gpt-oss-20b","openai/gpt-oss-120b"]

llm_selection = st.sidebar.selectbox("Select Model", model_choices)
temperature = st.sidebar.slider("Temperature", min_value=0.00, max_value=1.00, value=0.70)

session_id = st.text_input("Session ID", value="default_session")

retriever = None

# ------------------- FILE HANDLING & MODE SPECIFIC UI -------------------
if mode == "PDF":
Â  Â  uploaded_files = st.file_uploader("Upload PDFs", type="pdf", accept_multiple_files=True)
Â  Â  if uploaded_files:
Â  Â  Â  Â  with st.spinner("Processing PDFs..."):
Â  Â  Â  Â  Â  Â  @st.cache_resource(show_spinner=False)
Â  Â  Â  Â  Â  Â  def process_pdfs(_uploaded_files):
Â  Â  Â  Â  Â  Â  Â  Â  documents = []
Â  Â  Â  Â  Â  Â  Â  Â  for file in _uploaded_files:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temp_path = f"./{file.name}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with open(temp_path, "wb") as f: f.write(file.getvalue())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  loader = PyPDFLoader(temp_path)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  documents.extend(loader.load())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  os.remove(temp_path)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
Â  Â  Â  Â  Â  Â  Â  Â  splits = text_splitter.split_documents(documents)
Â  Â  Â  Â  Â  Â  Â  Â  vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
Â  Â  Â  Â  Â  Â  Â  Â  return vectorstore.as_retriever()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  retriever = process_pdfs(uploaded_files)
Â  Â  Â  Â  Â  Â  st.success("PDFs processed and ready!")

elif mode == "Image":
Â  Â  uploaded_image = st.file_uploader("Upload an Image", type=["jpg", "jpeg", "png"])
Â  Â  if uploaded_image:
Â  Â  Â  Â  st.image(uploaded_image, caption="Uploaded Image.", use_container_width=True)
Â  Â  Â  Â  st.session_state.image_bytes = uploaded_image.getvalue()

# ------------------- CHAT HISTORY MANAGEMENT -------------------

if "store" not in st.session_state:
Â  Â  st.session_state.store = {}

def get_session_history(session: str) -> BaseChatMessageHistory:
Â  Â  if session not in st.session_state.store:
Â  Â  Â  Â  st.session_state.store[session] = ChatMessageHistory()
Â  Â  return st.session_state.store[session]

history = get_session_history(session_id)


for message in history.messages:
Â  Â  with st.chat_message(message.type):
Â  Â  Â  Â  st.markdown(message.content)

# ------------------- CHAT INPUT AND RESPONSE LOGIC -------------------

if user_question := st.chat_input("Ask your question here..."):
Â  Â  # Display user message and add to history
Â  Â  with st.chat_message("human"):
Â  Â  Â  Â  st.markdown(user_question)
Â  Â  history.add_user_message(user_question)

Â  Â  llm_instance = get_llm_instance(llm_selection, temperature)
Â  Â  if llm_instance is None:
Â  Â  Â  Â  st.stop()

Â  Â  # Assistant's response logic
Â  Â  with st.chat_message("ai"):
Â  Â  Â  Â  response_placeholder = st.empty()
Â  Â  Â  Â  full_response = ""

Â  Â  Â  Â  if mode == "PDF" and retriever:
Â  Â  Â  Â  Â  Â  contextualize_q_prompt = ChatPromptTemplate.from_messages(
Â  Â  Â  Â  Â  Â  Â  Â  [("system", "Given a chat history and the latest user question... formulate a standalone question..."), MessagesPlaceholder("chat_history"), ("human", "{input}")]
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  history_aware_retriever = create_history_aware_retriever(llm_instance, retriever, contextualize_q_prompt)
Â  Â  Â  Â  Â  Â  qa_prompt = ChatPromptTemplate.from_messages(
Â  Â  Â  Â  Â  Â  Â  Â  [("system", "You are an assistant for question-answering tasks... Use the following retrieved context to answer...\n\n{context}"), MessagesPlaceholder("chat_history"), ("human", "{input}")]
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  question_answer_chain = create_stuff_documents_chain(llm_instance, qa_prompt)
Â  Â  Â  Â  Â  Â  rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  response_stream = rag_chain.stream(
Â  Â  Â  Â  Â  Â  Â  Â  {"input": user_question, "chat_history": history.messages}
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  def extract_answer(stream):
Â  Â  Â  Â  Â  Â  Â  Â  for chunk in stream:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'answer' in chunk: yield chunk['answer']
Â  Â  Â  Â  Â  Â  full_response = response_placeholder.write_stream(extract_answer(response_stream))

Â  Â  Â  Â  elif mode == "Image" and "image_bytes" in st.session_state:
Â  Â  Â  Â  Â  Â  message = HumanMessage(content=[
Â  Â  Â  Â  Â  Â  Â  Â  {"type": "text", "text": user_question},
Â  Â  Â  Â  Â  Â  Â  Â  {"type": "image_url", "image_url": f"data:image/jpeg;base64,{base64.b64encode(st.session_state.image_bytes).decode()}"}
Â  Â  Â  Â  Â  Â  ])
Â  Â  Â  Â  Â  Â  response_stream = llm_instance.stream([message])
Â  Â  Â  Â  Â  Â  def extract_content(stream):
Â  Â  Â  Â  Â  Â  Â  Â  for chunk in stream: yield chunk.content
Â  Â  Â  Â  Â  Â  full_response = response_placeholder.write_stream(extract_content(response_stream))

Â  Â  Â  Â  else:Â 
Â  Â  Â  Â  Â  Â  prompt = ChatPromptTemplate.from_messages([
Â  Â  Â  Â  Â  Â  Â  Â  ("system", "You are a helpful assistant. Answer the user's questions."),
Â  Â  Â  Â  Â  Â  Â  Â  MessagesPlaceholder(variable_name="chat_history"),
Â  Â  Â  Â  Â  Â  Â  Â  ("human", "{input}"),
Â  Â  Â  Â  Â  Â  ])
Â  Â  Â  Â  Â  Â  chain = prompt | llm_instance | StrOutputParser()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  response_stream = chain.stream(
Â  Â  Â  Â  Â  Â  Â  Â  {"input": user_question, "chat_history": history.messages}
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  full_response = response_placeholder.write_stream(response_stream)

Â  Â Â 
Â  Â  if full_response:
Â  Â  Â  Â  history.add_ai_message(full_response)

